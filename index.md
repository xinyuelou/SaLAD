---
layout: project_page
permalink: /

title:  "Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model"
authors:
  - "Xinyue Lou<sup>1</sup>, "
  - "You Li<sup>1</sup>, "
  - "Jinan Xu<sup>1</sup>, "
  - "Xiangyu Shi<sup>1</sup>, "
  - "Chi Chen<sup>2</sup>,"
  - "Kaiyu Huang<sup>1</sup>"
affiliations:
  - "<sup>1</sup>Beijing Jiaotong University, "
  - "<sup>2</sup>Tsinghua University "
paper: https://arxiv.org/pdf/2505.06538
code: https://github.com/xinyuelou/Think-in-Safety
data: https://huggingface.co/datasets/Holly301/Think-in-Safety
---

<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Abstract</h2>
        <div class="content has-text-justified">
The rapid development of Multimodal Large Reasoning Models (MLRMs) has demonstrated broad application potential, yet their safety and reliability remain critical concerns that require systematic exploration. To address this gap, we conduct a comprehensive and systematic safety evaluation of 13 MLRMs across 5 benchmarks and unveil prevalent safety degradation phenomena in most advanced models. Moreover, our analysis reveals distinct safety patterns across different benchmarks: significant safety degradation is observed across jailbreak robustness benchmarks, whereas safety-awareness benchmarks demonstrate less pronounced degradation. In particular, the long thought process in some scenarios even enhances safety performance. Therefore, it is a potential approach to address safety issues in MLRMs by leveraging the intrinsic reasoning capabilities of the model to detect unsafe intent. To operationalize this insight, we construct a multimodal tuning dataset that incorporates a safety-oriented thought process. Experimental results from fine-tuning existing MLRMs with this dataset effectively enhance the safety on both jailbreak robustness and safety-awareness benchmarks. This study provides a new perspective for developing safe MLRMs.
        </div>
    </div>
</div>

---

<p align="middle">
  <img src="static/image/first.jpg" width="400" />
  <img src="static/image/table1.jpg" width="800" /> 
</p>


Figure: **Up**, Examples of multimodal safety benchmarks and their corresponding responses on different models. **Down**, Variation of safety performance for MLRMs across various benchmarks.



## Contributions
1. This study conducts a systematic safety evaluation of MLRMs and investigates the empirical results, revealing several novel findings and providing new perspectives for the development of safer MLRMs.
2. We construct a multimodal fine-tuning dataset with safety-oriented thought process for safety alignment, alleviating the issue associated with incorporating additional modalities.
3. Experimental results demonstrate that our method improves the safety performance of MLRMs across multiple benchmarks by enabling self-correction thinking along the reasoning pathways, compared with previous defense methods.

## TiS Dataset

we employ a multi-stage pipeline to construct our safety alignment dataset TiS. We begin by collecting safety-related topics and generating image captions, then explicitly incorporating long CoT reasoning into question answering. After a filtering procedure, we finally obtain the dataset. To the best of our knowledge, TiS is the first safety dataset with the ability to retain reasoning chain for MLRMs.

<p align="middle">
  <img src="static/image/pipeline.jpg" width="800" />
</p>

Figure: Overview of our data construction pipeline.

## Results
Fine-tuning on TiS can significantly improve the safety of MLRMs while maintaining the thought process.

### Experimental Results
<p align="middle">
  <img src="static/image/table2.jpg" width="800" />
</p>
Both R1-Onevision and LLaVA-CoT demonstrate improved safety alignment fine-tuned on TiS, substantially outperforming prior datasets.

### Qualitative Results
<p align="middle">
  <img src="static/image/result.jpg" width="800" />
</p>
Examples of responses generated by fine-tuned on VLGuard, MIS, SPA-VL and TiS dataset are illustrated in Figure. Our approach demonstrates the ability to retain the thought process of the models while decisively rejecting unsafe inputs and explicitly articulating the potential serious consequences associated with such queries.



## Citation
```
@article{lou2025think,
  title={Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model},
  author={Lou, Xinyue and Li, You and Xu, Jinan and Shi, Xiangyu and Chen, Chi and Huang, Kaiyu},
  journal={arXiv preprint arXiv:2505.06538},
  year={2025}
}
```
