---
layout: project_page
permalink: /

title:  "When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life"
authors:
  - "Xinyue Lou<sup>1</sup>, "
  - "Jinan Xu<sup>1</sup>, "
  - "Jingyi Yin<sup>1</sup>, "
  - "Xiaolong Wang<sup>2</sup>, "
  - "Zhaolu Kang<sup>3</sup>, "
  - "Youwei Liao<sup>1</sup>, "
  - "Yixuan Wang<sup>1</sup>, "
  - "Xiangyu Shi<sup>1</sup>, "
  - "Fengran Mo<sup>4</sup>,"
  - "Su Yao<sup>2</sup>,"
  - "Kaiyu Huang<sup>1</sup>"
affiliations:
  - "<sup>1</sup>Beijing Jiaotong University, "
  - "<sup>2</sup>Tsinghua University "
  - "<sup>3</sup>Peking University "
  - "<sup>4</sup>University of Montreal"
paper: http://arxiv.org/abs/2601.04043
code: https://github.com/xinyuelou/SaLAD
data: https://huggingface.co/datasets/Holly301/SaLAD
---

<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Abstract</h2>
        <div class="content has-text-justified">
As Multimodal Large Language Models~(MLLMs) become an indispensable assistant in human life, the unsafe content generated by MLLMs poses a danger to human behavior, perpetually overhanging human society like a sword of Damocles. To investigate and evaluate the safety impact of MLLMs' responses on human behavior in daily life, we introduce SaLAD, a multimodal safety benchmark which contains 2,013 real-world image–text samples across 10 common categories, with a balanced design covering both unsafe scenarios and cases of oversensitivity. It emphasizes realistic risk exposure, authentic visual inputs, and fine-grained cross-modal reasoning, ensuring that safety risks cannot be inferred from text alone. We further propose a safety-warning-based evaluation framework that encourages models to provide clear and informative safety warnings, rather than generic refusals. Results on 18 MLLMs demonstrate that the top-performing models achieve a safe response rate of only 57.2\% on unsafe queries. Moreover, even popular safety alignment methods limit effectiveness of the models in our scenario, revealing the vulnerabilities of current MLLMs in identifying dangerous behaviors in daily life. 
        </div>
    </div>
</div>

---

<p align="middle">
  <img src="static/image/first.jpg" width="400" />
  <img src="static/image/table1.jpg" width="800" /> 
</p>


Figure: **Up**, Examples of multimodal safety benchmarks and their corresponding responses on different models. **Down**, Variation of safety performance for MLRMs across various benchmarks.



## Contributions
1. We propose a benchmark SaLAD towards the Safety of Multimodal LLM-Powered Assistant in Human Daily Life and adopt a safety-warning-based evaluation method, fo-
cusing on exploring the safety of model’s response regarding human behavior.
2. Evaluation results of 18 MLLMs demonstrate that existing models face challenges in detecting safety risks in human daily life, highlighting the critical safety vulnerability, especially in scenarios with images.
3. Further analysis reveals that employing traditional safety defense methods, including safety prompts and safety alignment methods, is not effective in addressing SaLAD, with underlying challenges remaining.

## SaLAD

we employ a multi-stage pipeline to construct our safety alignment dataset TiS. We begin by collecting safety-related topics and generating image captions, then explicitly incorporating long CoT reasoning into question answering. After a filtering procedure, we finally obtain the dataset. To the best of our knowledge, TiS is the first safety dataset with the ability to retain reasoning chain for MLRMs.

<p align="middle">
  <img src="static/image/pipeline.jpg" width="800" />
</p>

Figure: Overview of our data construction pipeline.

## Results
Fine-tuning on TiS can significantly improve the safety of MLRMs while maintaining the thought process.

### Experimental Results
<p align="middle">
  <img src="static/image/table2.jpg" width="800" />
</p>
Both R1-Onevision and LLaVA-CoT demonstrate improved safety alignment fine-tuned on TiS, substantially outperforming prior datasets.

### Qualitative Results
<p align="middle">
  <img src="static/image/result.jpg" width="800" />
</p>
Examples of responses generated by fine-tuned on VLGuard, MIS, SPA-VL and TiS dataset are illustrated in Figure. Our approach demonstrates the ability to retain the thought process of the models while decisively rejecting unsafe inputs and explicitly articulating the potential serious consequences associated with such queries.



## Citation
```
@misc{lou2026helpershazardsbenchmarkanalyzing,
      title={When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life}, 
      author={Xinyue Lou and Jinan Xu and Jingyi Yin and Xiaolong Wang and Zhaolu Kang and Youwei Liao and Yixuan Wang and Xiangyu Shi and Fengran Mo and Su Yao and Kaiyu Huang},
      year={2026},
      eprint={2601.04043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2601.04043}, 
}
```
